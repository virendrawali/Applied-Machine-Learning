{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "woZuGLfokoCx"
   },
   "source": [
    "# CSCI-P556\n",
    "# Assignment 1\n",
    "# Due date: Friday, February 15, 11:59PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5d1GxwAzsRfB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mCxVSEY_ku5i"
   },
   "source": [
    "## Question 1 (10 points)\n",
    "\n",
    "Implement linear regression using ordinary least squares (closed-form solution)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DQevk_umkdh3"
   },
   "outputs": [],
   "source": [
    "def linear_regression_ols(X, y):\n",
    "  # Make sure that you return the weights in a np.array, \n",
    "  # other data types will cause our grading script to crash\n",
    "  mat_len = len(X)\n",
    "  X = np.insert(X,0,np.ones(mat_len),axis=1)\n",
    "  X_transpose = np.transpose(X)\n",
    "  mat_mul = np.matmul(X_transpose,X)\n",
    "  inverse = np.linalg.inv(mat_mul)\n",
    "  mat_mul = np.matmul(inverse,X_transpose)\n",
    "  result = np.matmul(mat_mul,y)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R_CofGkGmgR6"
   },
   "source": [
    "## Question 2 (40 points)\n",
    "\n",
    "Implement linear regression using gradient descent. A boolean parameter named *regularization* has been included in the function definition. If regularization=True, then the linear regression will be computed using L2 regularization.\n",
    "\n",
    "![L2 regularization](https://cdn-images-1.medium.com/max/1200/1*jgWOhDiGjVp-NCSPa5abmg.png)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lCS581-dm3PV"
   },
   "outputs": [],
   "source": [
    "def linear_regression_gd(X, y, theta, \\\n",
    "                         learning_rate, \\\n",
    "                         num_iters, \\\n",
    "                         regularization=False):\n",
    "  # Make sure that you return the weights in a np.array, \n",
    "  # other data types will cause our grading script to crash\n",
    "  error_plot = []\n",
    "  lamb = 3\n",
    "  y = y.reshape(-1,1)\n",
    "  for epoch in range(num_iters):\n",
    "    y_hat = np.dot(x,np.transpose(theta))\n",
    "    delta = np.dot(np.transpose((y_hat - y)),x)\n",
    "    if(regularization == False):\n",
    "        theta = theta - (learning_rate * delta)/len(x)\n",
    "    else:\n",
    "        theta - (learning_rate*delta)/len(x) - (learning_rate * lamb * 2 * theta)/len(x)\n",
    "  return theta\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VV6PVnE0n4eb"
   },
   "source": [
    "## Question 3 (20 points)\n",
    "\n",
    "- Apply your linear regression OLS, gradient descent without regularization, and gradient descent with regularization functions to [Sci-Kit Learn's diabetes dataset](https://www.programcreek.com/python/example/85913/sklearn.datasets.load_diabetes). Additionally, apply [Sci-Kit Learn's Linear Regression model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "- Calculate the amount of time it took for each of the functions to execute with the code that we have included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mFVwHrYfpKbI"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zlwnxRsarM0M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total execution time for OLS: 0.0\n",
      "Total execution time for gradient descent without regularization: 0.09375\n",
      "Total execution time for gradient descent with regularization: 0.0\n",
      "Total execution time for Sci-Kit Learn's Linear Regression: 0.0\n"
     ]
    }
   ],
   "source": [
    "# load variables in this area\n",
    "# X = \n",
    "# y = \n",
    "\n",
    "# Used tab separated file from https://www4.stat.ncsu.edu/~boos/var.select/diabetes.tab.txt \n",
    "#and used local file path to read through pandas \n",
    "\n",
    "df = pd.read_csv('diabetes.txt', sep='\\t') \n",
    "\n",
    "arr = np.array(df)\n",
    "x = arr[:,:10]\n",
    "y = arr[:,10:]\n",
    "  \n",
    "start_time = time.process_time()\n",
    "updated_theta = linear_regression_ols(x, y)\n",
    "end_time = time.process_time()\n",
    "print(\"Total execution time for OLS: \" + str(end_time - start_time))\n",
    "\n",
    "learning_rate = 0.00001\n",
    "num_iters = 800\n",
    "x = np.insert(x,0,np.ones(len(x)),axis=1)\n",
    "theta = np.random.rand(len(x[0])).reshape(1,-1)\n",
    "\n",
    "start_time = time.process_time()\n",
    "updated_theta = linear_regression_gd(x, y, theta,learning_rate, num_iters,regularization=False)\n",
    "end_time = time.process_time()\n",
    "print(\"Total execution time for gradient descent without regularization: \" + str(end_time - start_time))\n",
    "\n",
    "\n",
    "start_time = time.process_time()\n",
    "updated_theta = linear_regression_gd(x, y, theta,learning_rate, num_iters,regularization=True)\n",
    "end_time = time.process_time()\n",
    "print(\"Total execution time for gradient descent with regularization: \" + str(end_time - start_time))\n",
    "\n",
    "\n",
    "start_time = time.process_time()\n",
    "# Do SK's linear regression here\n",
    "reg = LinearRegression().fit(x,y)\n",
    "coeff = reg.coef_\n",
    "end_time = time.process_time()\n",
    "print(\"Total execution time for Sci-Kit Learn's Linear Regression: \" + str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J7p2lIOzc5T-"
   },
   "source": [
    "## Question 4 (20 points)\n",
    "\n",
    "Normalize the appropriate variables in the dataset and re-do Question 3 using this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ON8HYSCjc2Hv"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('diabetes.txt', sep='\\t')\n",
    "arr = np.array(df)\n",
    "x = arr[:,:10]\n",
    "x = ((x - x.mean())/x.std()) # standardization i.e. using mean and standard deviation to normalize the values.\n",
    "y = arr[:,10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QfN0z1ZjdcHk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total execution time for OLS: 0.0\n",
      "Total execution time for gradient descent without regularization: 0.09375\n",
      "Total execution time for gradient descent with regularization: 0.09375\n",
      "Total execution time for Sci-Kit Learn's Linear Regression: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Make sure to use the normalized variables here, not the original ones.\n",
    "\n",
    "start_time = time.process_time()\n",
    "updated_theta = linear_regression_ols(x, y)\n",
    "end_time = time.process_time()\n",
    "print(\"Total execution time for OLS: \" + str(start_time-end_time))\n",
    "\n",
    "\n",
    "learning_rate = 0.1\n",
    "num_iters = 1500\n",
    "x = np.insert(x,0,np.ones(len(x)),axis=1)\n",
    "theta = np.random.rand(len(x[0])).reshape(1,-1)\n",
    "\n",
    "start_time = time.process_time()\n",
    "updated_theta = linear_regression_gd(x, y, theta,learning_rate, num_iters,regularization=False)\n",
    "end_time = time.process_time()\n",
    "print(\"Total execution time for gradient descent without regularization: \" + str(end_time -  start_time))\n",
    "\n",
    "\n",
    "start_time = time.process_time()\n",
    "updated_theta = linear_regression_gd(x, y, theta,learning_rate, num_iters,regularization=True)\n",
    "end_time = time.process_time()\n",
    "print(\"Total execution time for gradient descent with regularization: \" + str(end_time - start_time))\n",
    "\n",
    "\n",
    "start_time = time.process_time()\n",
    "reg = LinearRegression().fit(x,y)\n",
    "coeff = reg.coef_\n",
    "end_time = time.process_time()\n",
    "print(\"Total execution time for Sci-Kit Learn's Linear Regression: \" + str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I8gPX8LruFKw"
   },
   "source": [
    "## Question 5 (10 points, 2 points per quesiton)\n",
    "\n",
    "1. Did you notice any difference between the normalize and non-normalized versions in questions 3 and 4? Explain your answer.\n",
    "2. Which is the linear regressions is faster? Why is it faster?\n",
    "3. Why don't we train all the machine learning models using that technique?\n",
    "4. Describe in your own words at least two regularization methods\n",
    "5. What would happen if you use a regularization parameter value that is too low or too high?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JJevEyH8KPWZ"
   },
   "source": [
    "Write your answers here:\n",
    "\n",
    "1. Normalized data converge faster than non-normalized data, as scale in non-normalized data is different. Normalised data is      taking less number of iteration to reach minima while non-normalized data is taking more epoch. Also when I used larger      learning parameter in non-normalized data I am getting diverging graph i.e. overshooting the optimum value of weights.\n",
    "\n",
    "2. Ordinary least square method is faster than gradient descent because in gradient descent, we need to update the weights in      each iteration. Also in current dataset we have 10 features and 442 training examples which are relatively less, so OLS of    closed form is faster than gradient descent.\n",
    "\n",
    "3. Ordinary least square with closed form method is suitable for data having less features with less numbers of traning            examples. As we take inverse of matrix in OLS which takes (n)^3, so as number of features and traning examples increases      the inverse operation in closed form become time consuming and also it's difficult to find complex model with OLS.\n",
    "\n",
    "4. Ridge Regularization: In this regularization, we add sum squared values of weights as a penalty to loss function. It is              differentiable and help to reduce weight nearer to 0 which helps to reduce variance.\n",
    "\n",
    "   Lasso Regularization: In lasso regression, we add sum of absolute values of weights as a penalty to loss function. This regression    is usefull in feature selection as it helps to remove some features completely by making their weights 0.\n",
    "   \n",
    "   Elastic-net regulazation: This is a combination Ridge and Lasso regularization, we use another hyperparameter to to control    the contribution of Ridge and Lasso regularization to penalize weights.\n",
    "\n",
    "5. We use redularization parameter to control the comlexity of model i.e. variance by penalizing weights. If the value of          parameter is too low then model will be complex which is susceptible to overfit the data. On the other hand, if regularized    parameter value is too high then we are penalising the weights more i.e. lowering the weights which results into simpler      model which can underfit the data.\n",
    "\n",
    "\n",
    "References:\n",
    "\n",
    "https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c\n",
    "\n",
    "https://www.youtube.com/watch?v=IyDwQNXDWns\n",
    "\n",
    "https://www.robertoreif.com/blog/2017/12/21/importance-of-feature-scaling-in-data-modeling-part-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "P556-Assignment1-S19.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
