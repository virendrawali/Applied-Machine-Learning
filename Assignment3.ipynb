{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nVz1A_-srqo0"
   },
   "source": [
    "# Assignment 3\n",
    "# CSCI-P556, Spring 2019\n",
    "## Due date: Monday, March 25, 2019, by 11:59PM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bT2n-ypxrwhb"
   },
   "source": [
    "## Problem 1: Building a Neural Networks using Tensorflow's Low-Level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QCmIcvmYuAfH"
   },
   "source": [
    "The most popular deep learning library is Tensorflow. This library offers two ways develop deep learning models, these are through the High-Level API (Keras) and the Low-Level API. Keras' API is really straight-forward, below we include [Tensorflow's example](https://www.tensorflow.org/tutorials/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AKlB49oqq_p5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "8vaIneq4rMjA",
    "outputId": "aa4314e9-b16f-42cb-eba3-40cf87325cf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "8keTVIJSrQR1",
    "outputId": "735d7fb1-895f-47df-a81d-23cc2e84c9db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EKUiE0_hrRdF"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test, y_test)\n",
    "'''\n",
    "import os\n",
    "import tarfile\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "tfile=tarfile.open('kkanji.tar')\n",
    "tfile.extractall('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FKHdfXkEt_xk"
   },
   "source": [
    "Easy, right? Well, we are going to get our hands dirty with the low-level API. **Your task for this problem is to implement a fully-connected deep neural network using [Tensorflow's low-level API](https://www.tensorflow.org/guide).** Make sure that you code can handle passing parameters for the number of layers and hidden units, as well as any other parameter that you deem necessary.\n",
    "\n",
    "Note: we will take into account your models' accuracy when we grade. For example, if the average accuracy in the class is 91% accuracy and your model is giving 70% accuracy, then you did something wrong. If you aren't getting a good accuracy, check if your loss function is the correct one or try tweaking the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "id": "138XiGKQxK1h",
    "outputId": "e04d5b40-49c4-4dde-9619-bd9e6a99b924"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'kmnist' already exists and is not an empty directory.\n",
      "Please select a download option:\n",
      "1) Kuzushiji-MNIST (10 classes, 28x28, 70k examples)\n",
      "2) Kuzushiji-49 (49 classes, 28x28, 270k examples)\n",
      "3) Kuzushiji-Kanji (3832 classes, 64x64, 140k examples)\n",
      "> 2\n",
      "Please select a download option:\n",
      "1) NumPy data format (.npz)\n",
      "> 1\n",
      "Downloading k49-train-imgs.npz - 64.6 MB\n",
      "100% 64569/64569 [00:09<00:00, 6481.64KB/s]\n",
      "Downloading k49-train-labels.npz - 0.2 MB\n",
      "100% 161/161 [00:00<00:00, 258.89KB/s]\n",
      "Downloading k49-test-imgs.npz - 10.7 MB\n",
      "100% 10715/10715 [00:02<00:00, 3582.95KB/s]\n",
      "Downloading k49-test-labels.npz - 0.0 MB\n",
      "100% 27/27 [00:00<00:00, 172.63KB/s]\n",
      "All dataset files downloaded!\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/rois-codh/kmnist\n",
    "!python \"kmnist//download_data.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6-g7QFzsxVrz"
   },
   "source": [
    "We will be using two versions of [Kuzushiji dataset](https://arxiv.org/abs/1812.01718) to test your models: Kuzushiji-49 and Kuzushiji-Kanji. The descriptions of datasets are:\n",
    "\n",
    "> \"Kuzushiji-49, as the name suggests, has 49 classes (28x28 grayscale, 270,912 images), is a much larger, but imbalanced dataset containing 48 Hiragana characters and one Hiragana iteration mark.\"\n",
    "\n",
    "> \"Kuzushiji-Kanji is an imbalanced dataset of total 3832 Kanji characters (64x64 grayscale, 140,426 images), ranging from 1,766 examples to only a single example per class.\"\n",
    "\n",
    "If you read the description carefully, you can notice that these aren't datasets that we can just feed to the model without doing some pre-processing. Make sure that your code can handle this issue. Also, the number of layers and hidden units, as well as the hyperparameters are up to you to figure out. \n",
    "\n",
    "Instructions on how to download these datasets are available in their [Github page](https://github.com/rois-codh/kmnist). To clone Kuzushiji repository from Github to Google Colab, use:\n",
    "\n",
    "!git clone https://github.com/rois-codh/kmnist\n",
    "\n",
    "Note: all Linux commands are available in Google Colab by preceding them with a \"!\", just like we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hs2EXL1fz48t"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PKA9k7smiYpV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import class_weight\n",
    "    \n",
    "def weight_bias_calc(p1,p2):\n",
    "    weights =tf.Variable(tf.random_normal([p1,p2]))\n",
    "    bias = tf.Variable(tf.random_normal([p2]))\n",
    "    return weights,bias\n",
    "\n",
    "def NN_model(x,data):\n",
    "    for i in range(0,(len(data)-1)):\n",
    "        w,b = weight_bias_calc(data[i],data[i+1])\n",
    "        layer = tf.add(tf.matmul(x, w),b)\n",
    "        if(0< i <len(data)-2):\n",
    "            layer = tf.nn.relu(layer)\n",
    "        x = layer\n",
    "    return layer\n",
    "\n",
    "def train_neural_network(x,y,NN_layers_units,iterations,classes,train_images, train_labels, test_images, test_labels, batch_size,class_weights):\n",
    "    batch_x = None\n",
    "    batch_y = None\n",
    "    prediction = NN_model(x,NN_layers_units)\n",
    "    scaled_logits = tf.multiply(prediction,class_weights)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=scaled_logits, labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for itr in range(iterations):\n",
    "            loss = 0\n",
    "            for n in range(int(len(train_images) / batch_size)):\n",
    "                if(n == int(len(train_images) / batch_size)-1):\n",
    "                    batch_x = train_images[n*batch_size:]\n",
    "                    batch_y = train_labels[n*batch_size:]\n",
    "                else:\n",
    "                    batch_x = train_images[n*batch_size:(n+1)*batch_size]\n",
    "                    batch_y = train_labels[n*batch_size:(n+1)*batch_size]\n",
    "                feed_dict={x: batch_x,y: batch_y}\n",
    "                temp, c = sess.run([optimizer, cost], feed_dict)\n",
    "                loss += c\n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        print('Accuracy:', accuracy.eval({x: test_images, y:test_labels}))\n",
    "\n",
    "def create_NN_model(classes, train_images, train_labels, test_images, test_labels,class_weights, hidden_units = 1500,number_of_hidden_units = 4,batch_size = 100,number_of_iterations = 20):\n",
    "    x = tf.placeholder('float', [None, len(train_images[0])])\n",
    "    y = tf.placeholder('float',[None, classes])\n",
    "    NN_layers_units = []\n",
    "    NN_layers_units.append(len(train_images[0]))\n",
    "    for n in range(number_of_hidden_units):\n",
    "        NN_layers_units.append(hidden_units)\n",
    "    NN_layers_units.append(classes)\n",
    "    train_neural_network(x,y,NN_layers_units,number_of_iterations,classes,train_images, train_labels, test_images, test_labels, batch_size,class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dRpoS8ZHig16"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J4I3KnALgrdL"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "def data_preprocess_kkanji(classes):\n",
    "      x1 = []\n",
    "      y1 = []\n",
    "\n",
    "      number_of_files = []\n",
    "      for folder in os.listdir('kkanji2'):\n",
    "            number_of_files.append(len(os.listdir('kkanji2/'+folder)))\n",
    "      number_of_files = sorted(number_of_files, reverse = True)\n",
    "      val = number_of_files[classes]\n",
    "      index1 = 0\n",
    "      i = 0\n",
    "      \n",
    "      for folder in os.listdir('kkanji2'):\n",
    "          if(len(os.listdir('kkanji2/'+folder)) > val):\n",
    "            for file in os.listdir('kkanji2/'+folder):\n",
    "                img=Image.open('kkanji2/'+folder+'/'+file)\n",
    "                nomalized_image = np.array(img)/255\n",
    "                y1.append(index1)\n",
    "                x1.append(nomalized_image.reshape(nomalized_image.shape[0]*nomalized_image.shape[1]))\n",
    "                img.close()\n",
    "            index1+=1\n",
    "\n",
    "      train_images, test_images, train_labels, test_labels = train_test_split(x1, y1, test_size=0.15, random_state=0, shuffle = True, stratify = y1)\n",
    "      \n",
    "      class_weights = class_weight.compute_class_weight('balanced', np.unique(train_labels),train_labels)\n",
    "\n",
    "      train_images = np.array(train_images)\n",
    "      train_labels = np.array(train_labels)\n",
    "      test_images = np.array(test_images)\n",
    "      test_labels = np.array(test_labels)\n",
    "\n",
    "      sess = tf.Session()\n",
    "      train_labels = sess.run(tf.one_hot(np.array(train_labels),classes))\n",
    "      test_labels = sess.run(tf.one_hot(np.array(test_labels),classes))\n",
    "      sess.close()\n",
    "      return train_images, train_labels, test_images, test_labels,class_weights\n",
    "\n",
    "classes = 30\n",
    "train_images, train_labels, test_images, test_labels, class_weights = data_preprocess_kkanji(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-2GO0wmghxVx",
    "outputId": "146dff30-00b6-4d04-f85b-b73076b75b82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.84139305\n"
     ]
    }
   ],
   "source": [
    "hidden_units = 1500\n",
    "number_of_hidden_units = 4\n",
    "batch_size = 100\n",
    "number_of_iterations = 20\n",
    "\n",
    "create_NN_model(classes, train_images, train_labels, test_images, test_labels,class_weights, hidden_units,number_of_hidden_units,batch_size,number_of_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qGQJHKzij9fA"
   },
   "outputs": [],
   "source": [
    "\n",
    "def data_preprocess_k49():\n",
    "      classes = 49\n",
    "      train_images = np.load('k49-train-imgs.npz')['arr_0']\n",
    "      train_labels = np.load('k49-train-labels.npz')['arr_0']\n",
    "      test_images = np.load('k49-test-imgs.npz')['arr_0']\n",
    "      test_labels = np.load('k49-test-labels.npz')['arr_0']\n",
    "\n",
    "      class_weights = class_weight.compute_class_weight('balanced', np.unique(train_labels),train_labels)\n",
    "      train_images = train_images / 255.0\n",
    "      test_images = test_images / 255.0\n",
    "\n",
    "      x = tf.placeholder('float', [None, len(train_images[0])*len(train_images[0])])\n",
    "      y = tf.placeholder('float',[None, classes])\n",
    "\n",
    "      sess = tf.Session()\n",
    "      train_images = np.array(train_images).reshape(len(train_images),len(train_images[0]) * len(train_images[0]))\n",
    "      train_labels = sess.run(tf.one_hot(train_labels,classes))\n",
    "      test_images = np.array(test_images).reshape(len(test_images),len(test_images[0]) * len(test_images[0]))\n",
    "      test_labels = sess.run(tf.one_hot(test_labels,classes))\n",
    "      sess.close()\n",
    "      return train_images, train_labels, test_images, test_labels,class_weights\n",
    "\n",
    "train_images, train_labels, test_images, test_labels,class_weights = data_preprocess_k49()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "colab_type": "code",
    "id": "7yT6izOPxOqU",
    "outputId": "a9304bf3-eda4-4be7-84a7-bcd02d7f9b96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 completed out of 20 loss: 3365300910.46875\n",
      "Epoch 1 completed out of 20 loss: 699150604.8203125\n",
      "Epoch 2 completed out of 20 loss: 369090364.46484375\n",
      "Epoch 3 completed out of 20 loss: 230430987.76171875\n",
      "Epoch 4 completed out of 20 loss: 171206752.20336914\n",
      "Epoch 5 completed out of 20 loss: 118174574.94042969\n",
      "Epoch 6 completed out of 20 loss: 95260966.03979492\n",
      "Epoch 7 completed out of 20 loss: 75993976.12489319\n",
      "Epoch 8 completed out of 20 loss: 60589653.88265991\n",
      "Epoch 9 completed out of 20 loss: 54507414.18860245\n",
      "Epoch 10 completed out of 20 loss: 47260893.67675173\n",
      "Epoch 11 completed out of 20 loss: 40279340.329435825\n",
      "Epoch 12 completed out of 20 loss: 38473414.762096405\n",
      "Epoch 13 completed out of 20 loss: 34226559.517930984\n",
      "Epoch 14 completed out of 20 loss: 33760288.6743927\n",
      "Epoch 15 completed out of 20 loss: 28763907.132243156\n",
      "Epoch 16 completed out of 20 loss: 28957125.530597687\n",
      "Epoch 17 completed out of 20 loss: 25109377.592643738\n",
      "Epoch 18 completed out of 20 loss: 29403007.499087334\n",
      "Epoch 19 completed out of 20 loss: 21945917.831794024\n",
      "Accuracy: 0.7711106\n"
     ]
    }
   ],
   "source": [
    "classes = 49\n",
    "hidden_units = 1500\n",
    "number_of_hidden_units = 4\n",
    "batch_size = 100\n",
    "number_of_iterations = 20\n",
    "\n",
    "create_NN_model(classes, train_images, train_labels, test_images, test_labels,class_weights, hidden_units,number_of_hidden_units,batch_size,number_of_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pUya5ncqymwh"
   },
   "outputs": [],
   "source": [
    "# Test your model using Kuzushiji-Kanji here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wMDKQ9yWsS41"
   },
   "source": [
    "## Problem 2: Building a Decision Tree using Scikit-Learn's DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sli8nPfoz_0R"
   },
   "source": [
    "Repeat the above experiments using [Scikit-Learn's DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pJSXe-szrWQk",
    "outputId": "092d6c4a-694b-4aa2-e155-e4301b24ca97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.072716424105636\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "hidden_units = 512\n",
    "batch_size = 100\n",
    "\n",
    "train_images = np.load('k49-train-imgs.npz')['arr_0']\n",
    "train_labels = np.load('k49-train-labels.npz')['arr_0']\n",
    "test_images = np.load('k49-test-imgs.npz')['arr_0']\n",
    "test_labels = np.load('k49-test-labels.npz')['arr_0']\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(train_labels),train_labels)\n",
    "feed_dict = {}\n",
    "for i in range(0,len(class_weights)):\n",
    "    feed_dict[i] = class_weights[i]\n",
    "\n",
    "sess = tf.Session()\n",
    "train_images = np.array(train_images).reshape(len(train_images),len(train_images[0]) * len(train_images[0]))\n",
    "train_images = np.array(train_images).astype(float)\n",
    "test_images = np.array(test_images).reshape(len(test_images),len(test_images[0]) * len(test_images[0]))\n",
    "test_images = np.array(test_images).astype(float)\n",
    "sess.close()\n",
    "\n",
    "\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "model = tree.DecisionTreeClassifier(class_weight = feed_dict)\n",
    "model.fit(train_images, train_labels)\n",
    "prediction = model.predict(test_images)\n",
    "\n",
    "print(accuracy_score(test_labels, prediction)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "waJGedMr4EJb",
    "outputId": "7cdd890f-9108-4778-a55e-1cc00dff40bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.45607808340728\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import class_weight\n",
    "import os\n",
    "import tarfile\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "classes = 30\n",
    "\n",
    "x1 = []\n",
    "y1 = []\n",
    "\n",
    "number_of_files = []\n",
    "for folder in os.listdir('kkanji2'):\n",
    "    number_of_files.append(len(os.listdir('kkanji2/'+folder)))\n",
    "\n",
    "number_of_files = sorted(number_of_files, reverse = True)\n",
    "val = number_of_files[classes]\n",
    "\n",
    "index1 = 0\n",
    "\n",
    "for folder in os.listdir('kkanji2'):\n",
    "    if(len(os.listdir('kkanji2/'+folder)) > val):\n",
    "      for file in os.listdir('kkanji2/'+folder):\n",
    "          img=Image.open('kkanji2/'+folder+'/'+file)\n",
    "          normalized_image = np.array(img)/255\n",
    "          y1.append(index1)\n",
    "          x1.append(normalized_image.reshape(normalized_image.shape[0]*normalized_image.shape[1]))\n",
    "          img.close()\n",
    "      index1+=1\n",
    "\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(x1, y1, test_size=0.15, random_state=0, shuffle = True, stratify = y1)\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(train_labels),train_labels)\n",
    "\n",
    "\n",
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)\n",
    "test_images = np.array(test_images)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "feed_dict = {}\n",
    "for i in range(0,len(class_weights)):\n",
    "    feed_dict[i] = class_weights[i]\n",
    "\n",
    "model = tree.DecisionTreeClassifier(class_weight = feed_dict)\n",
    "\n",
    "model.fit(train_images, train_labels)\n",
    "prediction = model.predict(test_images)\n",
    "\n",
    "print(accuracy_score(test_labels, prediction)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qu_Y7_cesVdK"
   },
   "source": [
    "## Problem 3: Building a Gradient Boosted Tree using Tensorflow's BoostedTreeClassifier estimator\n",
    "\n",
    "In class we covered the vanilla decision tree. In recent years, a new kind of tree has become very popular in Kaggle competitions due to their performance, these trees are known as [Gradient Boosted Trees](https://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting). Typically, the winning submissions for most competitions make use of either some neural network or a gradient boosted tree (either, [XGBoost](https://xgboost.readthedocs.io/en/latest/) or [LightGBM](https://lightgbm.readthedocs.io/en/latest/)). For this problem, we will use [Tensorflow's BoostedTreeClassifier estimator](https://www.tensorflow.org/api_docs/python/tf/estimator/BoostedTreesClassifier). Repeat the above experiments using this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zGep6jYhsSCZ",
    "outputId": "9b67e499-8714-49d1-c4e9-aff52669bacb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5617817210159026\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "hidden_units = 512\n",
    "batch_size = 100\n",
    "\n",
    "classes = 49\n",
    "\n",
    "train_images = np.load('k49-train-imgs.npz')['arr_0']\n",
    "train_labels = np.load('k49-train-labels.npz')['arr_0']\n",
    "test_images = np.load('k49-test-imgs.npz')['arr_0']\n",
    "test_labels = np.load('k49-test-labels.npz')['arr_0']\n",
    "\n",
    "sess = tf.Session()\n",
    "train_images = np.array(train_images).reshape(len(train_images),len(train_images[0]) * len(train_images[0]))\n",
    "train_images = np.array(train_images).astype(float)\n",
    "test_images = np.array(test_images).reshape(len(test_images),len(test_images[0]) * len(test_images[0]))\n",
    "test_images = np.array(test_images).astype(float)\n",
    "sess.close()\n",
    "\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "train_data = lgb.Dataset(train_images,label=train_labels)\n",
    "\n",
    "params = dict()\n",
    "params['boosting_type'] = 'gbdt'\n",
    "params['application'] = 'multiclass'\n",
    "params['metric'] = 'multi_logloss'\n",
    "params['num_class'] = 49\n",
    "params['is_unbalanced'] = 'true'\n",
    "params['learning_rate'] = 0.05\n",
    "params['num_leaves'] = 10\n",
    "params['min_data_in_leaf'] = 1000\n",
    "params['min_sum_hessian_in_leaf'] = 1e-3\n",
    "params['max_depth'] = 10\n",
    "params['max_bin'] = 256\n",
    "params['feature_fraction'] = 0.7\n",
    "params['bagging_fraction'] = 0.7\n",
    "params['bagging_freq'] = 20\n",
    "\n",
    "num_round=50\n",
    "lgbm = lgb.train(params,train_data,num_round)\n",
    "predictions=lgbm.predict(test_images)\n",
    "result = []\n",
    "for p in predictions:\n",
    "    result.append(np.argmax(p))\n",
    "accuracy_lgbm = accuracy_score(test_labels,result)\n",
    "print(accuracy_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "oOc7_hTX4FuW",
    "outputId": "db946ec0-9d44-4507-8156-1a5e569486ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625\n",
      "0.7888198757763976\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score \n",
    "import os\n",
    "import tarfile\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "hidden_units = 512\n",
    "batch_size = 100\n",
    "\n",
    "x1 = []\n",
    "y1 = []\n",
    "\n",
    "classes = 30\n",
    "\n",
    "i = 0\n",
    "\n",
    "number_of_files = []\n",
    "for folder in os.listdir('kkanji2'):\n",
    "    number_of_files.append(len(os.listdir('kkanji2/'+folder)))\n",
    "\n",
    "number_of_files = sorted(number_of_files, reverse = True)\n",
    "val = number_of_files[classes]\n",
    "\n",
    "index1 = 0\n",
    "\n",
    "for folder in os.listdir('kkanji2'):\n",
    "    if(len(os.listdir('kkanji2/'+folder)) > val):\n",
    "      for file in os.listdir('kkanji2/'+folder):\n",
    "          img=Image.open('kkanji2/'+folder+'/'+file)\n",
    "          pix = np.array(img)/255\n",
    "          y1.append(i)\n",
    "          x1.append(pix.reshape(pix.shape[0]*pix.shape[1]))\n",
    "          img.close()\n",
    "      i+=1\n",
    "      \n",
    "train_images, test_images, train_labels, test_labels = train_test_split(x1, y1, test_size=0.15, random_state=0, shuffle = True, stratify = y1)\n",
    "      \n",
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)\n",
    "test_images = np.array(test_images)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "train_data = lgb.Dataset(train_images,label=train_labels)\n",
    "\n",
    "params = dict()\n",
    "params['boosting_type'] = 'gbdt'\n",
    "params['application'] = 'multiclass'\n",
    "params['metric'] = 'multi_logloss'\n",
    "params['num_class'] = classes\n",
    "params['is_unbalanced'] = 'true'\n",
    "params['learning_rate'] = 0.05\n",
    "params['num_leaves'] = 10\n",
    "params['min_data_in_leaf'] = 1000\n",
    "params['min_sum_hessian_in_leaf'] = 1e-3\n",
    "params['max_depth'] = 10\n",
    "params['max_bin'] = 256\n",
    "params['feature_fraction'] = 0.7\n",
    "params['bagging_fraction'] = 0.7\n",
    "params['bagging_freq'] = 20\n",
    "\n",
    "num_round=50\n",
    "lgbm = lgb.train(params,train_data,num_round)\n",
    "predictions=lgbm.predict(test_images)\n",
    "result = []\n",
    "for p in predictions:\n",
    "    result.append(np.argmax(p))\n",
    "accuracy_lgbm = accuracy_score(test_labels,result)\n",
    "print(accuracy_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I3IJvgVKGOEb"
   },
   "outputs": [],
   "source": [
    "#References\n",
    "\n",
    "#(1) https://www.youtube.com/watch?v=yX8KuPZCAMo\n",
    "#(2) https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18\n",
    "#(3) https://lightgbm.readthedocs.io/en/latest/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment3_P556_S19.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
